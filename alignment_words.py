import os
import json
import argparse
import pickle
import pathlib
import torch
from transformers import AutoTokenizer,AutoModel
from tqdm import tqdm
import networkx as nx
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from functools import partial


os.environ["CUDA_VISIBLE_DEVICES"]="5,6" # "4,5"
# ---------------------
# è·å– embedding çš„å‡½æ•° (è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…ä¸­æ›¿æ¢ä¸ºä½ çš„embeddingæ¨¡å‹)
# 3. æ‰¹é‡è·å–embedding
def get_embeddings(words_list,model,tokenizer, batch_size,device):
    embeddings = []

    for i in tqdm(range(0, len(words_list), batch_size)):
        batch_words = words_list[i:i+batch_size]

        # ç¼–ç æ–‡æœ¬
        inputs = tokenizer(batch_words, padding=True, truncation=True,
                           return_tensors="pt", max_length=16)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)

        # å– [CLS] å‘é‡ä½œä¸ºå¥å­/æ ‡ç­¾çš„embedding
        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)
        embeddings.append(cls_embeddings.cpu())

    # æ‹¼æ¥ç»“æœ
    return torch.cat(embeddings, dim=0)

def _process_pairs(pair_chunk, words_list,word2idx, embeddings, p1, p2):
    """å¤„ç†ä¸€æ‰¹ (i,j) å¯¹ï¼Œè¿”å›è¾¹åˆ—è¡¨"""
    edges = []
    for i, j in pair_chunk:
        # è®¡ç®—ç›¸ä¼¼åº¦
        word_a = words_list[i]
        e1 = embeddings[word2idx[word_a]].reshape(1, -1)
        word_b = words_list[j]
        e2 = embeddings[word2idx[word_b]].reshape(1, -1)

        sim = cosine_similarity(e1, e2)[0, 0]

        if 0.0 < sim < p1:
            relation = "isolated"
        elif p1 <= sim < p2:
            relation = "alias"
        elif p2 <= sim <= 1.0:
            relation = "duplicate"
        else:
            continue
        edges.append((word_a, word_b, {"weight": sim, "relation": relation}))
    return edges

def build_label_graph_multiple(words_list, all_words2index,embeddings, p1=0.6, p2=0.9, workers=None, chunk_size=5000):
    """
    å¤šè¿›ç¨‹æ„å»ºæ ‡ç­¾å›¾
    """
    if workers is None:
        workers = max(1, cpu_count() - 1)

    n = len(words_list)
    # æ‰€æœ‰ (i, j) å¯¹
    pairs = [(i, j) for i in range(n) for j in range(i + 1, n)]

    # åˆ†å—
    chunks = [pairs[k:k + chunk_size] for k in range(0, len(pairs), chunk_size)]

    # å‡†å¤‡éƒ¨åˆ†å‡½æ•°ï¼Œé¿å… lambda
    worker_func = partial(_process_pairs, words_list=words_list,word2idx=all_words2index, embeddings=embeddings, p1=p1, p2=p2)

    all_edges = []
    with Pool(processes=workers) as pool:
        for result in pool.imap_unordered(worker_func, chunks):
            all_edges.extend(result)

    # ä¸€æ¬¡æ€§æ„å»ºå›¾
    G = nx.Graph()
    G.add_nodes_from(words_list)
    G.add_edges_from(all_edges)
    return G

# ---------------------
# åˆ†æå›¾
def analyze_graph(G):
    isolated_edges = [(u, v) for u, v, d in G.edges(data=True) if d["relation"] == "isolated"]
    alias_edges = [(u, v) for u, v, d in G.edges(data=True) if d["relation"] == "alias"]
    duplicate_edges = [(u, v) for u, v, d in G.edges(data=True) if d["relation"] == "duplicate"]

    print("ğŸ“Œ åˆ«åèšåˆç°‡ï¼š")
    alias_subgraph = G.edge_subgraph(alias_edges)
    for comp in nx.connected_components(alias_subgraph):
        print(" -", comp)

    print("\nğŸ“Œ éœ€è¦å»æ‰çš„é‡å¤æ ‡ç­¾ï¼š")
    for u, v in duplicate_edges:
        print(f" - {u} <-> {v}")

    print("\nğŸ“Œ å­¤ç«‹æ ‡ç­¾ï¼š")
    alias_and_dup_nodes = set(sum(alias_edges + duplicate_edges, ()))
    for node in G.nodes():
        if node not in alias_and_dup_nodes:
            print(" -", node)

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path",type=str,default="/mnt/data/kw/models/Qwen/Qwen3-32B")
    parser.add_argument("--config_path",type=str,default="./config")
    parser.add_argument("--batch_size",type=int,default=128)
    parser.add_argument("--embedding_model",type=str,default="/mnt/data/kw/models/google-bert/bert-large-cased")
    parser.add_argument("--result_path",type=str,default="./result")
    parser.add_argument("--level_name",type=str,default="third")
    args = parser.parse_args()
    return args
def main():
    args = get_args()
    model_name = pathlib.Path(args.model_path).name
    tags_name_list = ["third","second","first"]
    assert args.level_name in tags_name_list
    load_cluser_file = os.path.join(args.result_path,model_name,f"clusters_{args.level_name}.txt")
    # åŠ è½½æ•°æ®é›†
    all_result_dict = {}
    all_words_list = []
    with open(load_cluser_file,mode="r",encoding="utf-8") as rfp:
        for line in rfp:
            name,index = line.strip().split("\t")
            if index not in all_result_dict:
                all_result_dict[index] = []
            all_words_list.append(name)
            all_result_dict[index].append(name)
    all_words2index = {}
    for index,word in enumerate(all_words_list):
        all_words2index[word] = index
    # åŠ è½½embedding æ¨¡å‹ï¼Œè¿™é‡Œç”¨åˆ°çš„æ˜¯bert large embeddingæ¨¡å‹
    device_id = 0
    device = torch.device(f"cuda:{device_id}")
    bert_tokenizer = AutoTokenizer.from_pretrained(args.embedding_model)
    bert_model = AutoModel.from_pretrained(args.embedding_model).to(device)
    
    # è°ƒç”¨å‡½æ•°è·å– embedding
    word2embedding = get_embeddings(all_words_list,bert_model,bert_tokenizer,args.batch_size,device)
    del bert_tokenizer
    del bert_model
    print("Embedding çŸ©é˜µå¤§å°:", word2embedding.shape)
    
    graph_path = os.path.join(args.result_path,model_name,"graph")
    if not os.path.exists(graph_path):
        os.makedirs(graph_path)
    with os.scandir(graph_path) as it:
        if not any(it):
            # åˆ›å»ºå…³ç³»å›¾ï¼Œé˜ˆå€¼è®¾ç½®ä¸º0.5ï¼Œ0.9
            p1 = 0.65
            p2 = 0.92
            all_graph_list = []
            for index in tqdm(all_result_dict,desc="processing"):
                words_list = all_result_dict[index]
                G = build_label_graph_multiple(words_list,all_words2index,word2embedding,p1=p1, p2=p2)
                save_graph_file = os.path.join(graph_path,f"{index}_graph.gpickle")
                # ä¿å­˜
                with open(save_graph_file, 'wb') as f:
                    pickle.dump(G, f)
                all_graph_list.append(G)
        else:
            # è¯»å–
            all_graph_list = []
            for file_name in os.listdir(graph_path):
                load_graph_file = os.path.join(graph_path,file_name)
                # Load graph
                with open(load_graph_file, 'rb') as f:
                    G = pickle.load(f)
                all_graph_list.append(G)
    # åˆ†æå›¾
    # analyze_graph(G)
    # éšåä½¿ç”¨å¤§æ¨¡å‹å¯¹æ ‡ç­¾ä½“ç³»è¿›è¡Œä¼˜åŒ–å¤„ç†
    # é’ˆå¯¹äºæ¯ä¸ªç°‡ï¼Œâ‘  é‡å¤çš„æ ‡ç­¾è¯·å¤§æ¨¡å‹å½’çº³ä¸ºä¸€ä¸ªæ ‡ç­¾ï¼›â‘¡ åˆ«åæ ‡ç­¾åˆ™ç”¨å¤§æ¨¡å‹è¿›è¡Œåç§°ä¼˜åŒ–
    
if __name__ == "__main__":
    main()
    # G = build_label_graph(words_list, p1=0.3, p2=0.7)
    # analyze_graph(G)
